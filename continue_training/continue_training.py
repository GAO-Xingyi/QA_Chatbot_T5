import os
import re
import torch
import argparse
import numpy as np
from tqdm.auto import tqdm
from bert4torch.models import *
from torch.utils.data import DataLoader, Dataset
from transformers import MT5ForConditionalGeneration, BertTokenizer
from transformers import T5ForConditionalGeneration, AdamW
#from transformers import BertTokenizer
import jieba
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu
import nltk
from tqdm import tqdm

int_classes = int
string_classes = str

TORCH_MAJOR = int(torch.__version__.split('.')[0])
TORCH_MINOR = int(torch.__version__.split('.')[1])

if TORCH_MAJOR == 1 and TORCH_MINOR < 8:
    from torch._six import container_abcs
else:
    import collections.abc as container_abcs

import sys

sys.setrecursionlimit(10**6)


def load_data(filename):
    D = []
    with open(filename, encoding='utf-8') as f:
        for l in f.readlines():
            cur = l.strip().split('\t')
            try:
                if len(cur) == 2:
                    question, answer = cur[0], cur[1]
                    D.append({'question': question, 'answer': answer})
                elif len(cur) == 1:
                    answer = cur[0]
                    D.append({'answer': answer})
            except ValueError:
                print("Invalid data format: {}".format(cur))
    return D


def create_data(data, tokenizer, max_len_generate, term='train'):
    ret, flag = [], True
    for item in data:
        question = item.get('question', 'Default Question')
        answer = item['answer']

        question_ids = tokenizer.encode(
            question, max_length=max_len_generate, truncation='only_first')
        answer_ids = tokenizer.encode(
            answer, max_length=max_len_generate, truncation='only_first')

        if flag and term == 'train':
            flag = False
            print(question)
        if term == 'train':
            features = {'input_ids': question_ids,
                        'decoder_input_ids': answer_ids,
                        'attention_mask': [1] * len(question_ids),
                        'decoder_attention_mask': [1] * len(answer_ids),
                        'answer': answer
                        }
        elif term == 'dev':
            features = {'input_ids': question_ids,
                        'attention_mask': [1] * len(question_ids),
                        'answer': answer
                        }
        ret.append(features)
    return ret


class T5PegasusTokenizer(BertTokenizer):
    """ç»“åˆä¸­æ–‡ç‰¹ç‚¹å®Œå–„çš„Tokenizer
    åŸºäºè¯é¢—ç²’åº¦çš„åˆ†è¯ï¼Œå¦‚è¯è¡¨ä¸­æœªå‡ºç°ï¼Œå†è°ƒç”¨BERTåŸç”ŸTokenizer
    """

    def __init__(self, pre_tokenizer=lambda x: jieba.cut(x, HMM=False), *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pre_tokenizer = pre_tokenizer

    def _tokenize(self, text, *arg, **kwargs):
        split_tokens = []
        for text in self.pre_tokenizer(text):
            if text in self.vocab:
                split_tokens.append(text)
            else:
                split_tokens.extend(super()._tokenize(text))
        return split_tokens


class KeyDataset(Dataset):
    def __init__(self, dict_data):
        self.data = dict_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data[index]


def sequence_padding(inputs, length=None, padding=0):
    """Numpyå‡½æ•°ï¼Œå°†åºåˆ—paddingåˆ°åŒä¸€é•¿åº¦
    """
    if length is None:
        length = max([len(x) for x in inputs])

    pad_width = [(0, 0) for _ in np.shape(inputs[0])]
    outputs = []
    for x in inputs:
        x = x[:length]
        pad_width[0] = (0, length - len(x))
        x = np.pad(x, pad_width, 'constant', constant_values=padding)
        outputs.append(x)

    return np.array(outputs, dtype='int64')


def default_collate(batch):
    """ç»„batch
    å„ä¸ªæ•°æ®åŸŸåˆ†åˆ«è½¬æ¢ä¸ºtensorï¼Œtensorç¬¬ä¸€ä¸ªç»´åº¦ç­‰äºbatch_size
    """
    np_str_obj_array_pattern = re.compile(r'[SaUO]')
    default_collate_err_msg_format = (
        "default_collate: batch must contain tensors, numpy arrays, numbers, "
        "dicts or lists; found {}")

    # ä½¿ç”¨ torch.cat æ›¿ä»£ torch.stack
    if isinstance(batch[0], torch.Tensor):
        return torch.cat([x.view(1, *x.shape) for x in batch], dim=0)

    elem = batch[0]
    elem_type = type(elem)
    if elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \
            and elem_type.__name__ != 'string_':
        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':
            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:
                raise TypeError(default_collate_err_msg_format.format(elem.dtype))
            return default_collate([torch.as_tensor(b) for b in batch])
        elif elem.shape == ():  # scalars
            return torch.as_tensor(batch)
    elif isinstance(elem, float):
        return torch.tensor(batch, dtype=torch.float64)
    elif isinstance(elem, int_classes):
        return torch.tensor(batch, dtype=torch.long)
    elif isinstance(elem, string_classes):
        return batch
    elif isinstance(elem, container_abcs.Mapping):
        return {key: default_collate([d[key] for d in batch]) for key in elem}
    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple
        return elem_type(*(default_collate(samples)
                           for samples in zip(*batch)))
    elif isinstance(elem, container_abcs.Sequence):
        it = iter(batch)
        elem_size = len(next(it))
        if not all(len(elem) == elem_size for elem in it):
            batch = sequence_padding(batch)
        return default_collate([default_collate(elem) for elem in batch])
    elif isinstance(elem, torch.Tensor) and elem.numel() > 0:
        return elem.resize_(0)
    raise TypeError(default_collate_err_msg_format.format(elem_type))


def prepare_data(args, data_path, tokenizer, term='train'):
    """å‡†å¤‡batchæ•°æ®
    """
    data = load_data(data_path)
    data = create_data(data, tokenizer, args.max_len_generate, term)
    data = KeyDataset(data)
    data = DataLoader(data, batch_size=args.batch_size,
                      collate_fn=default_collate)
    return data.dataset  # è¿”å› DataLoader å¯¹è±¡çš„ dataset å±æ€§


def compute_bleu(references, hypotheses):
    """
    è®¡ç®—BLEUåˆ†æ•°
    """
    smooth = nltk.translate.bleu_score.SmoothingFunction()
    bleu_score = nltk.translate.bleu_score.corpus_bleu(references, hypotheses, smoothing_function=smooth.method1)
    return bleu_score




def train_model(model, adam, train_data, dev_data, tokenizer, device, args):
    if not os.path.exists(args.model_dir):
        os.mkdir(args.model_dir)

    best_bleu_score = 0
    for epoch in range(args.num_epoch):
        model.train()
        for i, cur in enumerate(tqdm(train_data, desc='Epoch {}:'.format(epoch))):
            cur = {k: v.to(device) for k, v in cur.items()}
            prob = model(**cur)[0]
            mask = cur['decoder_attention_mask'][:, 1:].reshape(-1).bool()
            prob = prob[:, :-1]
            prob = prob.reshape((-1, prob.size(-1)))[mask]
            labels = cur['decoder_input_ids'][:, 1:].reshape(-1)[mask]
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(prob, labels)
            if i % 100 == 0:
                print("Iter {}:  Training Loss: {}".format(i, loss.item()))
            loss.backward()
            adam.step()
            adam.zero_grad()

        # ä¿å­˜æ¨¡å‹
        if args.data_parallel and torch.cuda.is_available():
            torch.save(model.module, os.path.join(args.model_dir, 'summary_model'))
        else:
            torch.save(model, os.path.join(args.model_dir, 'summary_model'))

    # éªŒè¯
    model.eval()
    generated_responses = []
    reference_answers = []
    for feature in tqdm(dev_data):
        title = feature['answer']
        content = {k: v.to(device) for k, v in feature.items() if k != 'answer'}
        if args.data_parallel and torch.cuda.is_available():
            gen = model.module.generate(max_length=args.max_len_generate,
                                        eos_token_id=tokenizer.sep_token_id,
                                        decoder_start_token_id=tokenizer.cls_token_id,
                                        **content)
        else:
            gen = model.generate(max_length=args.max_len_generate,
                                 eos_token_id=tokenizer.sep_token_id,
                                 decoder_start_token_id=tokenizer.cls_token_id,
                                 **content)
        gen = tokenizer.batch_decode(gen, skip_special_tokens=True)
        gen = [item.replace(' ', '') for item in gen]
        generated_responses.extend(gen)
        reference_answers.extend([title])

    # è®¡ç®—BLEUåˆ†æ•°
    bleu_score = compute_bleu([reference_answers], [generated_responses])
    print("Validation BLEU Score: {}".format(bleu_score))



def init_argument():
    parser = argparse.ArgumentParser(description='t5-pegasus-chinese')
    parser.add_argument('--train_data', default='./update_dataset/caoling.tsv')
    parser.add_argument('--dev_data', default='./update_dataset/caoling.tsv')
    parser.add_argument('--pretrain_model', default='../t5_pegasus_pretrain')
    parser.add_argument('--model_dir', default='./continue_training_model')

    parser.add_argument('--num_epoch', default=1000, help='number of epoch')
    parser.add_argument('--batch_size', default=16, help='batch size')
    parser.add_argument('--lr', default=2e-4, help='learning rate')
    parser.add_argument('--data_parallel', default=False)
    parser.add_argument('--max_len', default=25, help='max length of inputs')
    parser.add_argument('--max_len_generate', default=350, help='max length of outputs')

    args = parser.parse_args()
    return args

'''
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
è¿™ä¸€å—çš„è¾“å…¥å°ºå¯¸bugç»ˆäºè§£å†³äº†
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
æ„Ÿå†’äº†ï¼Œä»Šå¤©å¤´å¥½ç–¼ï¼Œåˆå†·åˆçƒ­ğŸ˜°   
'''
def data_normalization(train_data):
    for cur in train_data:
        cur['input_ids'] = torch.tensor(cur['input_ids'])
        cur['attention_mask'] = torch.tensor(cur['attention_mask'])
        cur['decoder_input_ids'] = torch.tensor(cur['decoder_input_ids'])
        cur['decoder_attention_mask'] = torch.tensor(cur['decoder_attention_mask'])

    for cur in train_data:
        input_seq_len = cur['input_ids'].shape[0]
        decoder_seq_len = cur['decoder_input_ids'].shape[0]
        cur['input_ids'] = cur['input_ids'].unsqueeze(1).expand(-1, input_seq_len)
        cur['decoder_input_ids'] = cur['decoder_input_ids'].unsqueeze(1).expand(-1, decoder_seq_len)

    for cur in train_data:
        input_seq_len = cur['input_ids'].shape[0]
        decoder_seq_len = cur['decoder_input_ids'].shape[0]
        cur['attention_mask'] = cur['attention_mask'].unsqueeze(1).expand(-1, input_seq_len)
        cur['decoder_attention_mask'] = cur['decoder_attention_mask'].unsqueeze(1).expand(-1, decoder_seq_len)

    return train_data


def continue_train(model, optimizer, train_data, tokenizer, device, args):
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()

    for epoch in range(args.num_epoch):
        # åˆå§‹åŒ–æŸå¤±
        total_loss = 0.0

        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        for cur in tqdm(train_data, desc=f'Epoch {epoch + 1}/{args.num_epoch}'):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
            cur = {k: v.to(device) for k, v in cur.items() if k != 'answer'}

            # å‰å‘ä¼ æ’­
            outputs = model(**cur)
            logits = outputs.logits

            # é€‰æ‹©ä¸æ­£ç¡®ç±»åˆ«å¯¹åº”çš„logits
            logits = logits[:, :, :2]

            print("decoder_attention_mask shape",cur['decoder_attention_mask'].shape)

            # è·å–æ©ç ï¼Œä¿ç•™æœªå¡«å……éƒ¨åˆ†
            mask = cur['decoder_attention_mask'][:, 1:].reshape(-1).bool()

            print("Original Mask Shape:", cur['decoder_attention_mask'].shape)
            print("mask shape:", mask.shape)

            # è°ƒæ•´logitsçš„ç»´åº¦
            logits = logits.view(-1, logits.size(-1))
            print("logits shape:", logits.shape)

            # å°†æ©ç åº”ç”¨äºlogitså’Œlabels
            logits = logits[mask, :]

            labels = cur['decoder_input_ids'][:, 1:].reshape(-1)[mask.view(-1)]
            print("labels shape:", labels.shape)

            # æ£€æŸ¥ labels æ˜¯å¦ä¸ºç©º
            if labels.numel() == 0:
                continue

            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(logits, labels)

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            # ç´¯ç§¯æ€»æŸå¤±
            total_loss += loss.item()

        # æ‰“å°æ¯ä¸ªepochçš„å¹³å‡æŸå¤±
        average_loss = total_loss / len(train_data)
        print(f"Epoch {epoch + 1}/{args.num_epoch}, å¹³å‡æŸå¤±: {average_loss}")





def continue_training(model, optimizer, train_data, dev_data, tokenizer, device, args):
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()

    best_bleu_score = 0
    for epoch in range(args.num_epoch):
        # åˆå§‹åŒ–æŸå¤±
        total_loss = 0.0

        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        for cur in tqdm(train_data, desc=f'Epoch {epoch + 1}/{args.num_epoch}'):
            print(cur)

            # # å°†æ•°æ®ç§»åˆ°è®¾å¤‡ä¸Š
            cur.pop('answer')
            # cur = {k: torch.tensor(v) for k, v in cur.items()}
            cur = {k: v.to(device) for k, v in cur.items()}
            seq_length = cur['decoder_input_ids'].shape[1]
            print(cur['input_ids'].shape)
            print(cur['attention_mask'].shape)
            print(cur['decoder_input_ids'].shape)
            print(cur['decoder_attention_mask'].shape)
            print(seq_length)
            print(cur['decoder_input_ids'])
            # å‰å‘ä¼ æ’­
            # prob = model(**cur)[0]
            # mask = cur['decoder_attention_mask'][:, 1:].reshape(-1).bool()
            # prob = prob[:, :-1]
            # labels = cur['decoder_input_ids'][:, 1:seq_length].reshape(-1)[mask]

            outputs = model(**cur)
            prob = outputs.logits
            mask = cur['decoder_attention_mask'][:, 1:].reshape(prob.shape[0], -1)
            # # prob = prob[:, :-1]
            # prob = prob[:, :, :2]  # å– prob çš„ç¬¬äºŒä¸ªç»´åº¦çš„å‰ 2 ä¸ªå…ƒç´ 
            # # prob = prob.view(-1, 2)
            #
            # # labels = cur['decoder_input_ids'][:, 1:seq_length].reshape(prob.shape[0], -1)
            # labels = cur['decoder_input_ids'][:, 1:seq_length]
            #
            # # labels = labels.repeat_interleave(prob.shape[1], dim=1)[mask]  # å°† labels å¹³é“ºå¹¶é‡å¤
            # # labels = labels[:, :2]
            # # labels = labels.repeat_interleave(25000, dim=1)  # å°† labels å¹³é“ºå¹¶é‡å¤
            # # labels = labels[mask].reshape(-1, 2)
            # labels = labels[:3]

            # é€‰æ‹©ä¸æ­£ç¡®ç±»åˆ«å¯¹åº”çš„logits
            prob = prob[:, :, :2]
            labels = cur['decoder_input_ids'][:, 1:seq_length]
            # å±•å¹³logitså’Œlabels
            # prob = prob.reshape(-1, prob.size(-1))
            prob = prob.reshape(-1, prob.size(-1))
            labels = labels.reshape(-1)

            # åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºattentionæ˜¯å¦åº”ç”¨çš„æ©ç 
            mask = mask.reshape(-1)

            print(prob.shape)
            print(labels.shape)
            print(mask.shape)

            # å°†æ©ç åº”ç”¨äºlogitså’Œlabels
            prob = prob[mask]
            labels = labels[mask]

            print(prob.shape)
            print(type(prob))
            print(prob)
            print(labels.shape)
            print(type(labels))
            print(labels)

            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=10)
            print(labels.min(), labels.max())
            print(loss_fct)

            loss = loss_fct(prob, labels)


            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            # ç´¯ç§¯æ€»æŸå¤±
            total_loss += loss.item()

        # æ‰“å°æ¯ä¸ªepochçš„å¹³å‡æŸå¤±
        average_loss = total_loss / len(train_data)
        print(f"Epoch {epoch + 1}/{args.num_epoch}, å¹³å‡æŸå¤±: {average_loss}")

        # éªŒè¯
        model.eval()
        generated_responses = []
        reference_answers = []
        for feature in tqdm(dev_data):
            title = feature['answer']
            content = {k: v for k, v in feature.items() if k != 'answer'}
            gen = model.generate(max_length=args.max_len_generate,
                                        eos_token_id=tokenizer.sep_token_id,
                                        decoder_start_token_id=tokenizer.cls_token_id,
                                        **content)
            gen = tokenizer.batch_decode(gen, skip_special_tokens=True)
            gen = [item.replace(' ', '') for item in gen]
            generated_responses.extend(gen)
            reference_answers.extend([title])

        # è®¡ç®—BLEUåˆ†æ•°
        bleu_score = compute_bleu([reference_answers], [generated_responses])
        print("Validation BLEU Score: {}".format(bleu_score))

        # å¦‚æœå½“å‰BLEUåˆ†æ•°é«˜äºä¹‹å‰æœ€å¥½çš„åˆ†æ•°ï¼Œåˆ™ä¿å­˜æ¨¡å‹
        if bleu_score > best_bleu_score:
            best_bleu_score = bleu_score
            torch.save(model, os.path.join(args.model_dir, 'summary_model'))

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model, os.path.join(args.model_dir, 'summary_model_final'))






if __name__ == '__main__':
    args = init_argument()

    tokenizer = T5PegasusTokenizer.from_pretrained(args.pretrain_model)
    train_data = prepare_data(args, args.train_data, tokenizer, term='train')
    dev_data = prepare_data(args, args.dev_data, tokenizer, term='dev')
    print(type(train_data))
    print(type(dev_data))
    train_data = data_normalization(train_data)
    #dev_data = data_normalization(dev_data)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = MT5ForConditionalGeneration.from_pretrained(args.pretrain_model)
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)

    # ç»§ç»­è®­ç»ƒ
    # continue_training(model, train_data,tokenizer=tokenizer, num_epochs=args.num_epoch, learning_rate=args.lr)
    # continue_training(model,optimizer,train_data,dev_data, tokenizer, device, args)
    continue_train(model, optimizer, train_data, tokenizer, device, args)